ROUGE: A Package for Automatic Evaluation of Summaries
Chin-Yew Lin
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292
cyl@isi.edu
Abstract
ROUGE stands for Recall-Oriented Understudy for
Gisting Evaluation. It includes measures to automatically determine the quality of a summary by
comparing it to other (ideal) summaries created by
humans. The measures count the number of overlapping units such as n-gram, word sequences, and
word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different
ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W,
and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three
of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale
summarization evaluation sponsored by NIST.
1

Introduction

Traditionally evaluation of summarization involves
human judgments of different quality metrics, for
example, coherence, conciseness, grammaticality,
readability, and content (Mani, 2001). However,
even simple manual evaluation of summaries on a
large scale over a few linguistic quality questions
and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003)
would require over 3,000 hours of human efforts.
This is very expensive and difficult to conduct in a
frequent basis. Therefore, how to evaluate summaries automatically has drawn a lot of attention in the
summarization research community in recent years.
For example, Saggion et al. (2002) proposed three
content-based evaluation methods that measure
similarity between summaries. These methods are:
cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence. However,
they did not show how the results of these automatic
evaluation methods correlate to human judgments.
Following the successful applic ation of automatic
evaluation methods, such as BLEU (Papineni et al.,
2001), in machine translation evaluation, Lin and
Hovy (2003) showed that methods similar to BLEU ,

i.e. n-gram co-occurrence statistics, could be applied
to evaluate summaries. In this paper, we introduce a
package, ROUGE, for automatic evaluation of summaries and its evaluations. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It
includes several automatic evaluation methods that
measure the similarity between summaries. We describe ROUGE-N in Section 2, ROUGE-L in Section
3, ROUGE-W in Section 4, and ROUGE-S in Section
5. Section 6 shows how these measures correlate
with human judgments using DUC 2001, 2002, and
2003 data. Section 7 concludes this paper and discusses future directions.
2

ROUGE-N: N-gram Co-Occurrence Statistics

Formally, ROUGE-N is an n-gram recall between a
candidate summary and a set of reference summaries. ROUGE-N is computed as follows:
ROUGE-N

=

∑

∑ Count

S ∈{ ReferemceSummaries} gramn ∈ S

∑

match

( gramn )

∑ Count (gram )

(1)

n

S ∈{ ReferenceSummaries} gramn ∈ S

Where n stands for the length of the n-gram,
gramn , and Countmatch(gramn ) is the maximum number of n-grams co-occurring in a candidate summary
and a set of reference summaries.
It is clear that ROUGE-N is a recall-related measure because the denominator of the equation is the
total sum of the number of n-grams occurring at the
reference summary side. A closely related measure,
BLEU , used in automatic evaluation of machine
translation, is a precision-based measure. BLEU
measures how well a candidate translation matches
a set of reference translations by counting the percentage of n-grams in the candidate translation overlapping wit h the references. Please see Papineni et
al. (2001) for details about BLEU .
Note that the number of n-grams in the denominator of the ROUGE-N formula increases as we add
more references. This is intuitive and reasonable
because there might exist multiple good summaries.

Every time we add a reference into the pool, we expand the space of alternative summaries. By controlling what types of references we add to the
reference pool, we can design evaluations that focus
on different aspects of summarization. Also note
that the numerator sums over all reference summaries. This effectively gives more weight to matching
n-grams occurring in multiple references. Therefore
a candidate summary that contains words shared by
more references is favored by the ROUGE-N measure. This is again very intuitive and reasonable because we normally prefer a candidate summary that
is more similar to consensus among reference summaries.
2.1

Multiple References

So far, we only demonstrated how to compute
ROUGE-N using a single reference. When multiple
references are used, we compute pairwise summarylevel ROUGE-N between a candidate summary s and
every reference, ri , in the reference set. We then
take the maximum of pairwise summary-level
ROUGE-N scores as the final multiple reference
ROUGE-N score. This can be written as follows:
ROUGE-Nmulti = argmaxi ROUGE-N(ri ,s)
This procedure is also applied to computation of
ROUGE-L (Section 3), ROUGE-W (Section 4) , and
ROUGE-S (Section 5). In the implementation, we use
a Jackknifing procedure. Given M references, we
compute the best score over M sets of M-1 references. The final ROUGE-N score is the average of
the M ROUGE-N scores using different M-1 references. The Jackknifing procedure is adopted since
we often need to compare system and human performance and the reference summaries are usually
the only human summaries available. Using this
procedure, we are able to estimate average human
performance by averaging M ROUGE-N scores of
one reference vs. the rest M-1 references. Although
the Jackknif ing procedure is not necessary when we
just want to compute ROUGE scores using multiple
references, it is applied in all ROUGE score computations in the ROUGE evaluation package.
In the next section, we describe a ROUGE measure
based on longest common subsequences between
two summaries.
3

ROUGE-L: Longest Common Subs equence

A sequence Z = [z1 , z2, ..., zn ] is a subsequence of
another sequence X = [x1 , x2 , ..., x m ], if there exists a
strict increasing sequence [i1 , i2 , ..., ik] of indices of
X such that for all j = 1, 2, ..., k, we have xij = zj
(Cormen et al., 1989). Given two sequences X and
Y, the longest common subsequence (LCS) of X and

Y is a common subsequence with maximum length.
LCS has been used in identifying cognate candidates during construction of N-best translation lexicon from parallel text. Melamed (1995) used the
ratio (LCSR) between the length of the LCS of two
words and the length of the longer word of the two
words to measure the cognateness between them.
He used LCS as an approximate string matching
algorithm. Saggion et al. (2002) used normalized
pairwise LCS to compare simila rity between two
texts in automatic summarization evaluation.
3.1

Sentence-Level LCS

To apply LCS in summarization evaluation, we
view a summary sentence as a sequence of words.
The intuition is that the longer the LCS of two
summary sentences is, the more similar the two
summaries are. We propose using LCS-based Fmeasure to estimate the similarity between two
summaries X of length m and Y of length n, assuming X is a reference summary sentence and Y is a
candidate summary sentence, as follows:

LCS ( X , Y )
(2)
m
LCS ( X , Y )
Plcs =
(3)
n
(1 + β 2 ) Rlcs Plcs
Flcs =
(4)
Rlcs + β 2Plcs
Rlcs =

Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and ß = Plcs/Rlcs when
?Flcs/?Rlcs_ =_ ?Flcs/?Plcs . In DUC, ß is set to a very
big number (? 8 ). Therefore, only Rlcs is considered. We call the LCS-based F-measure, i.e. Equation 4, ROUGE-L. Notice that ROUGE-L is 1 when X
= Y; while ROUGE-L is zero when LCS(X,Y) = 0, i.e.
there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to have
met several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen, 1979). The composite factors are LCS-based
recall and precision in this case. Melamed et al.
(2003) used unigram F-measure to estimate machine
translation quality and showed that unigram Fmeasure was as good as BLEU .
One advantage of using LCS is that it does not require consecutive matches but in-sequence matches
that reflect sentence level word order as n-grams.
The other advantage is that it automatically includes
longest in-sequence common n-grams, therefore no
predefined n-gram length is necessary.
ROUGE-L as defined in Equation 4 has the property that its value is less than or equal to the min imum of unigram F-measure of X and Y. Unigram

