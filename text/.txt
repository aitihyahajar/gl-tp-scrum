Automatic Categorization of Author Gender

via N-Gram Analysis

Jonathan Doyle

Vlado Keˇselj

Faculty of Computer Science, Dalhousie University,

6050 University Avenue, Halifax, Nova Scotia, Canada

e-mail : {doyle,vlado}@cs.dal.ca

Abstract

We present a method for automatic
categorization of author gender via
n-gram analysis. Using a corpus
of British student essays, experi-
ments using character-level, word-
level, and part-of-speech n-grams are
performed. The peak accuracy for all
methods is roughly equal, reaching a
maximum of 81%. These results are
on par with other, established tech-
niques, while retaining the simplicity
and ease-of-generalization inherent in
n-gram techniques.

1 Introduction
There are diﬀerent subtasks of text classiﬁca-
tion and they can be divided into topic-based
and non-topic-based classiﬁcation. The tra-
ditional text classiﬁcation is topic-based and
a typical example is news classiﬁcation. Re-
cently, there has been an increasing activity in
the area of non-topic classiﬁcation as well, e.g.,
in sub-tasks such as

1. genre classiﬁcation (Finn and Kushmer-
ick, 2003), (E. Stamatatos and Kokki-
nakis, 2000),

2. sentiment classiﬁcation,

3. spam identiﬁcation,

4. language and encoding identiﬁcation, and

5. authorship attribution and plagiarism de-

tection (Khmelev and Teahan, 2003).

Many algorithms have been invented for as-
sessing the authorship of a given text. These
algorithms rely on the fact that authors use lin-
guistic devices at every level—semantic, syn-
tactic,
lexicographic, orthographic and mor-
phological (Ephratt, 1997)—to produce their

text. Typically, such devices are applied un-
consciously by the author, and thus provide
a useful basis for unambiguously determining
authorship. The most common approach to
determining authorship is to use stylistic anal-
ysis that proceeds in two steps: ﬁrst, spe-
ciﬁc style markers are extracted, and second,
a classiﬁcation procedure is applied to the re-
sulting description. These methods are usually
based on calculating lexical measures that rep-
resent the richness of the author’s vocabulary
and the frequency of common word use (Sta-
matatos et al., 2001). Style marker extraction
is usually accomplished by some form of non-
trivial NLP analysis, such as tagging, parsing
and morphological analysis. A classiﬁer is then
constructed, usually by ﬁrst performing a non-
trivial feature selection step that employs mu-
tual information or Chi-square testing to de-
termine relevant features.

However, there are several disadvantages of
this standard approach. First, techniques used
for style marker extraction are almost always
language dependent, and in fact diﬀer dramat-
ically from language to language. For example,
an English parser usually cannot be applied to
German or Chinese. Second, feature selection
is not a trivial process, and usually involves
setting thresholds to eliminate uninformative
features (Scott and Matwin, 1999). These de-
cisions can be extremely subtle, because al-
though rare features contribute less signal than
common features, they can still have an impor-
tant cumulative eﬀect (Aizawa, 2001). Third,
current authorship attribution systems invari-
ably perform their analysis at the word level.
However, although word level analysis seems
to be intuitive, it ignores the fact that mor-
phological features can also play an important
role, and moreover that many Asian languages
such as Chinese and Japanese do not have word
boundaries explicitly identiﬁed in text. In fact,

word segmentation itself is a diﬃcult prob-
lem in Asian languages, which creates an extra
level of diﬃculty in coping with the errors this
process introduces. Additionally, the number
of authors is small in all reported experiments,
so the size of author-speciﬁc information is not
an issue. If the number of authors, or classes in
general, is large, we have to set a limit on the
author-speciﬁc information, i.e., on the author
proﬁle.

In this paper, we propose a simple method

that avoids each of these problems.

Two important operations are:

1. choosing the optimal set of n-grams to be

included in the proﬁle, and

2. calculating the similarity between two

proﬁles.

The approach does not depend on a speciﬁc
language, and it does not require segmentation
for languages such as Chinese or Thai. There
is no any text preprocessing or higher level pro-
cessing required for character or word n-grams,
while the most complicated NLP tool used be-
ing a part-of-speech tagger used in two of the
experiments.

The small proﬁle size is not important only
for eﬃciency reasons, but it is also a natural
mechanism for over-ﬁtting control.

2 N-Gram Analysis

The term ‘N-gram’ refers to a series of sequen-
tial tokens in a document. The series can be
of length 1 (‘unigrams’), length 2 (‘bigrams’),
etc, towards the generalized term “N-gram”.
The tokens used can be words, letters, or any
other unit of information present throughout
the document. This versatility allows N-gram
analysis techniques to be applied to other me-
dia: both images (Rickman and Rosin, 1996)
and music (Doraisamy and Ruger, 2003) have
been the focus of N-gram research.

N-grams have been used in a wide variety of
situations, including optical character recog-
nition (Harding et al., 1997) and author at-
tribution (Keselj et al., 2003). The technique
involves the construction of a ‘proﬁle’ — es-
sentially a listing of the relative proportions of
each potential N-gram. When an item is to be
classiﬁed, its proﬁle is compared with known

ones to determine the best match. The ba-
sic method of comparison is an N-dimensional
distance measurement.

The use of n-gram probability distribution
and n-gram models in NLP is a relatively sim-
ple idea, but it has been found to be eﬀective
in many applications. For example, character
level n-gram language models can be easily ap-
plied to any language, and even non-language
sequences such as DNA and music. Character
level n-gram models are widely used in text
compression—e.g., the PPM model (T. Bell
and Witten, 1990)—and have recently been
found to be eﬀective in text mining problems
as well (I. Witten and Teahan, 1999). Text cat-
egorization with n-gram models has also been
attempted by (Cavnar and Trenkle, 1994).

3 Corpus
We used a collection of student essays from the
British Academic Written English (BAWE)
corpus (Nesi et al., 2004). Only the pilot data
for this corpus was available; it nominally con-
sisted of 500 essays, though not all of these
were suitable for inclusion. The metadata in-
cluded for each essay consisted of information
such as author gender, ﬁrst language, the grade
received etc.

Two essays were simply not present; oth-
ers did not have metadata present indicating
author gender. After these unacceptable es-
says were excluded, 495 were left in the set.
Within these, the average document length
was 2,812 words or 17,994 characters, with
1,391,710 words and 8,907,064 characters to-
tal.

4 Methodology
4.1 Proﬁle Generation
For each experiment, an individual proﬁle was
created for each document in the test set us-
ing the Perl module Text::NGrams. The cutoﬀ
point for each individual proﬁle was 100,000
N-grams; as no document had this number of
unique N-grams, this implies that the proﬁle
for each document was complete. Proﬁles were
created using character, word, and part-of-
speech tags as the tokens to be proﬁled. In the
latter case, an additional experiment was per-
formed after replacing non-function tags with
an asterisk. Proﬁles were generated for N-
grams of size 1 through 5 inclusive, with that

size being the limit of computational feasibil-
ity. No pre-processing of the data was per-
formed; the documents were left as found in
the corpus.

4.2 Part-of-Speech Tagging

Additional copies of the text were generated
with words replaced by their part-of-speech
tag. The tagging was performed automatically
using the Perl module Lingua::EN::Tagger, a
second-order Hidden Markov Model-based tag-
ger. A further copy of the text was made
with all non-function words removed, under
the assumption that treating content-bearing
words would not little meaning with respect
to style. They were arbitrarily replaced by an
asterisk. The speech categories considers as
function words were: prepositions, pronouns,
conjunctions, question adverbs (e.g.
‘when’),
interjections, and determiners.

4.3 Training and Testing Sets

20% each of the male and female lists, rounded
up, were randomly selected; these documents
would constitute the test set. There were 42
male and 58 female-authored texts in this set,
for a total of 100 essays. The remaining essays
were taken as the training set.

The ‘male’ and ‘female’ essays within this set
were listed, and for each list, the proﬁles of that
list’s members were combined. The combined
proﬁles were then normalized so as to have a
sum N-gram count equal to 1. See Table 1
for a sample of the data produced. This step
was performed for all N-gram sizes for which
proﬁles had been generated.

Table 1: Top ﬁve character bigrams from the
female training set, showing both normalized
and unnormalized values. Data has been
truncated for presentation.

4.4 Determining Closest Proﬁle
For each of the 100 documents in the test set, a
‘distance’ measurement was calculated to the
trained ‘male’ and ‘female’ proﬁles. The dis-
tance between two proﬁles was calculated as in
(Keselj et al., 2003); the exact formula is given

in equation 1.(cid:88)

(cid:181)

n  proﬁles

(cid:182)2

2(p1(n) − p2(n))
p1(n) + p2(n)

(1)

Lower distances indicate a closer match; for
each essay, the lower distance was printed as
the system’s guess. The output was recorded
and tested for accuracy, the results of which
can be found in the next section.

The experiment was repeated for various
proﬁle cutoﬀ lengths; in each case, the merged
test proﬁle was simply truncated after a given
number of entries and the distance measure-
ments re-run. Note that this will have no eﬀect
once the cutoﬀ length exceeds the maximum
proﬁle length, as there will be no items to be
truncated at that point.

5 Results

5.1 Character N-Grams
Both male and female authors had spaces as
their most frequently-used character, followed
by e,t,i, and a. Only minor diﬀerences fol-
lowed thereafter — the proﬁles were very sim-
ilar. This is to be expected, as are the poor
results for unigrams in this category.

An increase in the n size provided a notice-
able improvement, reaching a peak accuracy
of 76% is reached for an N of 4 and an L of
20,000.

Table 2: Results using character-based extrac-
tion

Normalized Unnormalized

Proﬁle Length

N-Gram Size

1

2

3

4

5

E
0.03274
T 0.02743
0.02480
S
TH 0.02277
A 0.01945

121221
101542
91827
84306
72001

100
1000
5000
10000
20000

51% 67% 58% 59% 58%
51% 69% 64% 63% 68%
51% 69% 74% 73% 70%
51% 69% 42% 74% 71%
51% 69% 42% 76% 72%

5.2 Word N-Grams
The female authors appeared to have a slightly
higher vocabulary than the male authors, with
unique word counts of 31734 and 30186 respec-
tively. The diﬀerent rises for word pairs, with
277769 unique word pairs within the female
training set, compared to 237417 in the male.
This eﬀect may be partially explained by the
larger number of female-authored documents.
In general, the word-based categorization
was highly successful, achieving a peak accu-
racy of 81% is reached for an N of 4 and an L
of 10,000–20,000.

Table 3: Results using word-based extraction

Proﬁle Length

N-Gram Size

1

2

3

4

5

100
1000
2000
5000
10000
15000

64% 62% 73% 71% 65%
70% 76% 72% 77% 74%
75% 75% 73% 77% 74%
74% 71% 74% 73% 74%
73% 71% 75% 81% 74%
73% 70% 73% 81% 77%

5.3 Part-of-Speech N-Grams
It has been suggested (Argamon et al., 2003)
that part-of-speech N-grams can ‘eﬃciently en-
code syntactical information’, and that this
may be of use in style classiﬁcation. This is
not unreasonable; the same source provides
evidence for gender-based trends in part-of-
speech tags. Speciﬁcally, the results for Table
5.3 shows the results for these. A peak accu-
racy of 76% is reached for an N of 5 and an
L of 5,000. This is roughly comparable to the
other results in this study.

Table 4: Results using part-of-speech extrac-
tion

Proﬁle Length

N-Gram Size

1

2

3

4

5

100
500
1000
2000
5000
10000

42% 64% 61% 52% 66%
42% 63% 68% 68% 64%
42% 62% 64% 66% 65%
42% 58% 69% 68% 70%
42% 58% 66% 71% 76%
42% 58% 67% 72% 74%

5.4 Function Word N-Grams
It has been also previously suggested that func-
tion words may be a strong determiner of au-
thor characteristics (Zhao and Zobel, 2005).
To test this, the experiment was run again on
proﬁles for which non-function words had been
replaced by an asterisk. The results of our test
may be seen in Table 5.4.

As with the full part-of-speech proﬁles, a
peak accuracy of 76% is reached. This time,
it is for an N of 4 and an L of 1,000. While
the peak is the same, overall accuracy is lower
than in Table 5.3.

Table 5: Results using part-of-speech extrac-
tion, with non-function words replaced by an
asterisk

Proﬁle Length

N-Gram Size

1

2

3

4

5

100
500
1000
2000
5000
10000

42% 60% 58% 62% 63%
42% 58% 72% 67% 61%
42% 58% 67% 76% 59%
42% 58% 64% 73% 59%
42% 58% 42% 72% 70%
42% 58% 42% 71% 72%

6 Conclusion
We have presented a method for automatic
identiﬁcation of author gender based on n-
gram proﬁles. The method is successful on this
corpus; it would be desirable to try it on oth-
ers to determine the versatility. Because no in-
formation speciﬁc to this experiment has been
used, it is likely that the techniques would be
equally-applicable to other data sets. Further,
the technique is not language-speciﬁc, suggest-
ing is is probably applicable across languages.
The use of part-of-speech tags had no sub-
stantial eﬀect on the results, showing only a
slight decrease. It is possible that with a more
accurate tagger better results would be found.
Although simple, in this case N-gram anal-
ysis performs on par with other techniques,
achieving a peak accuracy of 81%. For com-
parative purposes, (Koppel et al., 2002) claim
an accurate of ‘approximately 80%’.

Acknowledgments
We would like to thank the maintainers of the
BAWE corpus for providing access to the pilot

D. Khmelev and W. Teahan. 2003. A repetition
based measure for veriﬁcation of text collections
and for text categorization.
In SIGIR’2003,
Toronto, Canada.

Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Lin-
guistic Computing, 17(4):401–412.

and Lisa
Hilary Nesi, Gerard
Ganobcsik-Williams.
Student papers
across the curriculum: Designing and devel-
oping a corpus of british student writing.
Computers and Composition, 21(4):439–450.

Sharpling,
2004.

R. Rickman and P. Rosin. 1996. Content-based
image retrieval using colour n-grams. IEEE Col-
loquium on Intelligent Image Databases, pages
15/1–15/6.

S. Scott and S. Matwin. 1999. Feature engineering
for text classiﬁcation. In Proceedings ICML-99.

E. Stamatatos, N. Fakotakis, and G. Kokkinakis.
2001. Computer-based authorship attribution
without lexical measures. Computers and the
Humanities, 35:193–214.

J. Cleary T. Bell and I. Witten. 1990. Text Com-

pression. Prentice Hall.

Ying Zhao and Justin Zobel.

2005. Eﬀective
and scalable authorship attribution using func-
tion words. The 2nd Asia Information Retrieval
Symposium.

data used in this article. We would also like to
acknowledge the contribution of three anony-
mous reviewers, whose feedback has been help-
ful.

This research is supported by the Natural
Sciences and Engineering Research Council of
Canada.

References

A. Aizawa. 2001. Linguistic techniques to improve
the performance of automatic text categoriza-
tion. In Proceedings 6th NLP Pac. Rim Symp.
NLPRS-01.

Shlomo Argamon, Moshe Koppel, Jonathan Fine,
and Anat Rachel Shimoni. 2003. Gender, genre,
and writing style in formal written texts. Text,
23(3):321–346.

W. Cavnar and J. Trenkle. 1994. N-gram-based

text categorization. In Proceedings SDAIR-94.

Shyamala Doraisamy and Stefan Ruger.

2003.
Robust polyphonic music retrieval with n-
grams. Journal of Intelligent Information Sys-
tems, 21(1):53–70, July.

N. Fakotakis E. Stamatatos and G. Kokkinakis.
2000. Automatic text categorization in terms
of genre and author. Computational Linguistics,
26(4):471–495.

M. Ephratt. 1997. Authorship attribution - the
case of lexical innovations. In Proc. ACH-ALLC-
97.

Aidan Finn and Nicholas Kushmerick.

2003.
Learning to classify documents according to
genre. In IJCAI-03 Workshop on Computational
Approaches to Style Analysis and Synthesis.

Stephen M. Harding, W. Bruce Croft, and C. Weir.
1997. Probabilistic retrieval of ocr degraded text
using n-grams. Probabilistic Retrieval of OCR
Degraded Text Using N-Grams, 1324:345–359.

M. Mahoui I. Witten, Z. Bray and W. Teahan.
1999. Text mining: A new frontier for lossless
compression. In Proceedings of the IEEE Data
Compression Conference (DCC).

Vlado Keselj, Fuchun Peng, Nick Cercone, and
Calvin Thomas.
2003. N-gram-based author
proﬁles for authorship attribution. Proceedings
of the Conference Paciﬁc Association for Com-
putational Linguistics PACLING’03, August.

phrases that consists of several words to be formed. We evaluate the quality of the phrase
representations using a new analogical reasoning task that involves phrases. Table 2 shows examples
of the ﬁve categories of analogies used in this task. This dataset is publicly available on the web2.
4.1 Phrase Skip-Gram Results
Starting with the same news data as in the previous experiments, we ﬁrst constructed the phrase
based training corpus and then we trained several Skip-gram models using different hyper-
parameters. As before, we used vector dimensionality 300 and context size 5. This setting already
achieves good performance on the phrase dataset, and allowed us to quickly compare the Negative
Sampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens.
The results are summarized in Table 3.
The results show that while Negative Sampling achieves a respectable accuracy even with k = 5,
using k = 15 achieves considerably better performance. Surprisingly, while we found the Hierar-
chical Softmax to achieve lower performance when trained without subsampling, it became the best
performing method when we downsampled the frequent words. This shows that the subsampling
can result in faster training and can also improve accuracy, at least in some cases.

2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt

Dimensionality No subsampling [%]

Method
NEG-5
NEG-15

HS-Huffman

300
300
300

24
27
19

10−5 subsampling [%]

27
42
47

Table 3: Accuracies of the Skip-gram models on the phrase analogy dataset. The models were
trained on approximately one billion words from the news dataset.

6

NEG-15 with 10−5 subsampling HS with 10−5 subsampling

Vasco de Gama
Lake Baikal
Alan Bean
Ionian Sea
chess master

Lingsugur

Great Rift Valley
Rebbeca Naomi
chess grandmaster

Ruegen

Italian explorer
Aral Sea
moonwalker
Ionian Islands
Garry Kasparov

Table 4: Examples of the closest entities to the given short phrases, using two different models.
Czech + currency
French + actress
Juliette Binoche
Vanessa Paradis

Vietnam + capital
Ho Chi Minh City

German + airlines
airline Lufthansa
carrier Lufthansa

ﬂag carrier Lufthansa

Lufthansa

Charlotte Gainsbourg

Cecile De

koruna

Check crown
Polish zolty

CTK

Hanoi
Viet Nam
Vietnamese

Russian + river
Moscow
Volga River
upriver
Russia

Table 5: Vector compositionality using element-wise addition. Four closest tokens to the sum of two
vectors are shown, using the best Skip-gram model.

To maximize the accuracy on the phrase analogy task, we increased the amount of the training data
by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality
of 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy
of 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B
words, which suggests that the large amount of the training data is crucial.
To gain further insight into how different the representations learned by different models are, we did
inspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we
show a sample of such comparison. Consistently with the previous results, it seems that the best
representations of phrases are learned by a model with the hierarchical softmax and subsampling.
5 Additive Compositionality
We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit
a linear structure that makes it possible to perform precise analogical reasoning using simple vector
arithmetics. Interestingly, we found that the Skip-gram representations exhibit another kind of linear
structure that makes it possible to meaningfully combine words by an element-wise addition of their
vector representations. This phenomenon is illustrated in Table 5.
The additive property of the vectors can be explained by inspecting the training objective. The word
vectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors
are trained to predict the surrounding words in the sentence, the vectors can be seen as representing
the distribution of the context in which a word appears. These values are related logarithmically
to the probabilities computed by the output layer, so the sum of two word vectors is related to the
product of the two context distributions. The product works here as the AND function: words that
are assigned high probabilities by both word vectors will have high probability, and the other words
will have low probability. Thus, if “Volga River” appears frequently in the same sentence together
with the words “Russian” and “river”, the sum of these two word vectors will result in such a feature
vector that is close to the vector of “Volga River”.
6 Comparison to Published Word Representations
Many authors who previously worked on the neural network based representations of words have
published their resulting models for further use and comparison: amongst the most well known au-
thors are Collobert and Weston [2], Turian et al. [17], and Mnih and Hinton [10]. We downloaded
their word vectors from the web3. Mikolov et al. [8] have already evaluated these word representa-
tions on the word analogy task, where the Skip-gram models achieved the best performance with a
huge margin.

3http://metaoptimize.com/projects/wordreprs/

7

Model

(training time)
Collobert (50d)

(2 months)

Turian (200d)
(few weeks)

Mnih (100d)

(7 days)

Skip-Phrase
(1000d, 1 day)

Redmond

Havel

ninjutsu

grafﬁti

capitulate

conyers
lubbock
keene

McCarthy
Alston
Cousins
Podhurst
Harlang
Agarwal

Redmond Wash.

Redmond Washington

Microsoft

plauen
dzerzhinsky
osterreich
Jewell
Arzu
Ovitz
Pontiff
Pinochet
Rodionov
Vaclav Havel

president Vaclav Havel

Velvet Revolution

reiki
kohona
karate

-
-
-
-
-
-
ninja

martial arts
swordsmanship

cheesecake
gossip
dioramas
gunﬁre
emotion
impunity
anaesthetics
monkeys
Jews

spray paint
graﬁtti
taggers

abdicate
accede
rearm

-
-
-

Mavericks
planning
hesitated
capitulation
capitulated
capitulating

Table 6: Examples of the closest tokens given various well known models and the Skip-gram model
trained on phrases using over 30 billion training words. An empty cell means that the word was not
in the vocabulary.

To give more insight into the difference of the quality of the learned vectors, we provide empirical
comparison by showing the nearest neighboursof infrequentwords in Table 6. These examples show
that the big Skip-gram model trained on a large corpus visibly outperforms all the other models in
the quality of the learned representations. This can be attributed in part to the fact that this model
has been trained on about 30 billion words, which is about two to three orders of magnitude more
data than the typical size used in the prior work. Interestingly, although the training set is much
larger, the training time of the Skip-gram model is just a fraction of the time complexity required by
the previous model architectures.

7 Conclusion
This work has several key contributions. We show how to train distributed representations of words
and phrases with the Skip-gram model and demonstrate that these representations exhibit linear
structure that makes precise analogical reasoning possible. The techniques introduced in this paper
can be used also for training the continuous bag-of-words model introduced in [8].
We successfully trained models on several orders of magnitude more data than the previously pub-
lished models, thanks to the computationally efﬁcient model architecture. This results in a great
improvement in the quality of the learned word and phrase representations, especially for the rare
entities. We also found that the subsampling of the frequent words results in both faster training
and signiﬁcantly better representations of uncommon words. Another contribution of our paper is
the Negative sampling algorithm, which is an extremely simple training method that learns accurate
representations especially for frequent words.
The choice of the training algorithm and the hyper-parameter selection is a task speciﬁc decision,
as we found that different problems have different optimal hyperparameter conﬁgurations. In our
experiments, the most crucial decisions that affect the performance are the choice of the model
architecture, the size of the vectors, the subsampling rate, and the size of the training window.
A very interesting result of this work is that the word vectors can be somewhat meaningfully com-
bined using just simple vector addition. Another approach for learning representations of phrases
presented in this paper is to simply represent the phrases with a single token. Combination of these
two approaches gives a powerful yet simple way how to represent longer pieces of text, while hav-
ing minimal computational complexity. Our work can thus be seen as complementary to the existing
approach that attempts to represent phrases using recursive matrix-vector operations [16].
We made the code for training the word and phrase vectors based on the techniques described in this
paper available as an open-source project4.

4code.google.com/p/word2vec

8

References
[1] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language

model. The Journal of Machine Learning Research, 3:1137–1155, 2003.

[2] Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings of the 25th international conference on Machine
learning, pages 160–167. ACM, 2008.

[3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi-

ﬁcation: A deep learning approach. In ICML, 513–520, 2011.

[4] Michael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical mod-
els, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307–361,
2012.

[5] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of
recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011
IEEE International Conference on, pages 5528–5531. IEEE, 2011.

[6] Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for Training
Large Scale Neural Network Language Models. In Proc. Automatic Speech Recognition and Understand-
ing, 2011.

[7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno

[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations

University of Technology, 2012.

in vector space. ICLR Workshop, 2013.

[9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word

Representations. In Proceedings of NAACL HLT, 2013.

[10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in

neural information processing systems, 21:1081–1088, 2009.

[11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language

models. arXiv preprint arXiv:1206.6426, 2012.

[12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Pro-

ceedings of the international workshop on artiﬁcial intelligence and statistics, pages 246–252, 2005.

[13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by back-

propagating errors. Nature, 323(6088):533–536, 1986.

[14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.
[15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and
natural language with recursive neural networks. In Proceedings of the 26th International Conference on
Machine Learning (ICML), volume 2, 2011.

[16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality
Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods
in Natural Language Processing (EMNLP), 2012.

[17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 384–394. Association for Computational Linguistics, 2010.

[18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In

Journal of Artiﬁcial Intelligence Research, 37:141-188, 2010.

[19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.

In Transactions of the Association for Computational Linguistics (TACL), 353–366, 2013.

[20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota-
tion. In Proceedings of the Twenty-Second international joint conference on Artiﬁcial Intelligence-Volume
Volume Three, pages 2764–2770. AAAI Press, 2011.

9

